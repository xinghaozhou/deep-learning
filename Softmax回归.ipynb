{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d96a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归 vs 分类\n",
    "# 回归估计一个连续值（比如房价）\n",
    "# 分类预测一个连续的类别（比如一张图片里是猫还是狗）\n",
    "# 回归是单数值输出，输出区间是自然区间R，和真实值的区别作为损失\n",
    "# 分类通常有多个输出，输出i是预测第i类的置信度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d10b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从回归到多类分类 - 均方损失\n",
    "# 1. 对类别进行一位有效编码 y = [y1, y2,..., yn]^T\n",
    "#  yi = 1 if i = y, =0 otherwise. 也就是假如说真实类别是第i个,yi =1, 其他的元素全部等于0\n",
    "# 2. 使用均方损失训练\n",
    "# 3. 最大值最为预测 y_hat = argmax oi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4395c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们不关心实际的值，关心的是对 ‘正确类别的置信度很大’\n",
    "# 也就是需要更置信的识别正确类（大余量）\n",
    "# o_y - o_i >= delta(y,i), 也就是对正确类y的置信度远远大于其他非正确类的置信度，也就是差值大于某些阈值（这里为delta）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec0f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从回归到多类分类 - 校验比例\n",
    "# 1. 输出匹配概率（非负，和为1）\n",
    "# 2. y_hat = softmax(o), 这里的y_hat是长为n的向量，有我们要的属性\n",
    "# 3. y_hat_i = exp(o_i)/ sigma_k exp(o_k)\n",
    "# 4. 概率y和y_hat的区别作为损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0904c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax和交叉熵损失\n",
    "# 1. 交叉熵常用来衡量两个概率的区别 H(p,q) = sigma_i -p_i log(q_i)\n",
    "# 2. 将它作为损失 l(y, y_hat)= - sigma_i y_i log y_hat_i, 因为y_i里只有一个为1，剩下的为0，所以 = -log y_hat_y, \n",
    "#   也就是在分类，我们不关心对错误类的值，只关心对正确类的预测值\n",
    "# 3. 其梯度是真实概率和预测概率的区别 d/do_i l(y, y_hat) = softmax(o_i)-y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ad9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结\n",
    "# 1. Softmax回归是一个多类分类模型\n",
    "# 2. 使用Softmax操作子得到每个类的预测置信度\n",
    "# 3. 使用交叉熵来衡量预测和标号的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b02adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数种类\n",
    "# 1. L2 Loss 均方损失 l(y,y') = 1/2 (y-y')^2\n",
    "# 2. L1 Loss l(y,y') = |y-y'|, 当我们不需要很快的梯度下降时，缺点是零点处不可导\n",
    "# 3. Huber's Robust Loss 结合两者，当|y-y'|>1时用 |y-y'|-1/2; otherwise, 用L; 优点前期优化快，后期变慢，不会反复横跳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c66d6f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;66;03m#计算机视觉的库\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;66;03m#读取数据方便\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;66;03m#对数据进行操作\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision #计算机视觉的库\n",
    "from torch.utils import data #读取数据方便\n",
    "from torchvision import transforms #对数据进行操作\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.use_svg_display() #用于清晰地加载图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bcdb19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trans \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mToTensor() \u001b[38;5;66;03m#将图片转化成tensor的形式\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#这里将数据库保存在../data下, 训练数据，根据trans来调整数据库，下载数据\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mnist_train \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mFashionMNIST(\n\u001b[1;32m      4\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transforms\u001b[38;5;241m=\u001b[39mtrans, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "trans = transforms.ToTensor() #将图片转化成tensor的形式\n",
    "#这里将数据库保存在../data下, 训练数据，根据trans来调整数据库，下载数据\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, transforms=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, transforms=trans, download=True)\n",
    "len(mnist_train), len(mnist_test) #查看数据库大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e14bdc48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmnist_train\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist_train' is not defined"
     ]
    }
   ],
   "source": [
    "mnist_train[0][0].shape #[1,28,28] 这里第一个[0]表示第一个example, 第二个[0]表示图片, 如果是[1]就是标号\n",
    "#因为是黑白图片，所以rgb是1为channel, 28为大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890fdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_MNIST_labels(labels):\n",
    "    #返回Fashion MNIST数据集的文本标签\n",
    "    text_labels = [\n",
    "        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',\n",
    "        'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels] #返回各个标签的英文名\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, title=None, scale=1.5):\n",
    "    # 画东西\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            #图片张量\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            #PIL图片\n",
    "            ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8198c912",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mDataLoader(mnist_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)))\n\u001b[1;32m      2\u001b[0m show_images(X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m9\u001b[39m, titles \u001b[38;5;241m=\u001b[39m get_fashion_MNIST_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#几个样本的图像及其对应的标签\n",
    "X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\n",
    "show_images(X.reshape(18, 28, 28), 2, 9, titles = get_fashion_MNIST_labels(y)) #这里的18是num of examples, 2行，一行9张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841043da",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (719529669.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_dataloader_workers()\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "#读一个小批量数据，大小为batch_size\n",
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers()\n",
    "    #使用4个进程来读取的数据\n",
    "    return 4\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,  #训练数据需要随机\n",
    "                             num_workers=get_dataloader_workers()) #这里需要多少进程\n",
    "\n",
    "timer = d2l.Timer()\n",
    "#这里是访问所有数据的时间\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "f'{timer.stop():.2f} sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "511b04c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '，' (U+FF0C) (3474491034.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    num_workers=get_dataloader_workers())，\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '，' (U+FF0C)\n"
     ]
    }
   ],
   "source": [
    "#定义load_data_fashion_mnist函数\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transforms=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transforms=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,  \n",
    "                             num_workers=get_dataloader_workers())，\n",
    "             （data.DataLoader(mnist_train, batch_size, shuffle=False,  \n",
    "                             num_workers=get_dataloader_workers())）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7187577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7300de6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3926573232.py, line 183)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 183\u001b[0;36m\u001b[0m\n\u001b[0;31m    if nrows * ncols == 1:\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs = 784 #因为图片是28*28，我们需要将它拉长为一个向量\n",
    "num_output = 10 #因为输出是10个种类\n",
    "\n",
    "# WX+b \n",
    "# 这里W是训练的weight, 用normal随机初始化，W的size为784*10\n",
    "# 这里b是增加的bias，在最后输出时增加\n",
    "W = torch.normal(0, 0.01, size = (num_inputs, num_outputs), requires_grad=True) \n",
    "b = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "# 给定一个矩阵X，我们可以对所有元素求和\n",
    "# 举例 \n",
    "X = torch.tensor([1.0, 2.0, 3.0], [4.0, 5.0, 6.0])\n",
    "# 当X.sum(0)时表示，所有的行相加，列不变，也就是 [5.0, 7.0, 9.0] 为1x3的矩阵\n",
    "X.sum(0, keepdim=True)\n",
    "# 当X.sum(1)时表示，所以的列相加，行不变，也就是 [[6.0], [15.0]] 为2x1的矩阵\n",
    "X.sum(1, keepdim=True)\n",
    "\n",
    "#实现Softmax\n",
    "# Softmax(X)_ij = exp(X_ij) / sigma_k exp(X_ik)\n",
    "#   这里X可以理解为一个批次的logits(类别信心)，X_i就是这个批次中第i个example中各个类别的信心，X_ij就是‘预测样本X_i属于j类别的信心'\n",
    "#   exp(X_ij)确保输出的是正整数\n",
    "#   sigma_k exp(X_ik)是对第i个example的每个类别k的总和\n",
    "def softmax(x):\n",
    "    #这里计算了这个批次每个logits的指数\n",
    "    X_exp = torch.exp(X) \n",
    "    # 这里对指数化后的矩阵X用.sum将矩阵的行求和，变为一个1xn的矩阵，也就是说每行变成一个总和值\n",
    "    # 也就是说每一行就是所有类别指数化后的总和\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    #这里用到了广播机制，shape和X_exp一样\n",
    "    return X_exp / partition\n",
    "    # 举个例子\n",
    "    # X = torch.tensor([[1.0, 2.0, 3.0],\n",
    "    #              [1.0, -1.0, 0.0],\n",
    "    #              [-1.0, 1.0, 2.0]]) 是一个批次的logits\n",
    "    \n",
    "    # 第一步的结果\n",
    "    # tensor([[ 2.7183,  7.3891, 20.0855],\n",
    "    #         [ 2.7183,  0.3679,  1.0000],\n",
    "    #         [ 0.3679,  2.7183,  7.3891]])\n",
    "    \n",
    "    # 第二步的结果\n",
    "    # tensor([[30.1928],\n",
    "    #         [ 4.0862],\n",
    "    #         [10.4753]])\n",
    "\n",
    "    # 第三步的结果\n",
    "    # tensor([[0.0900, 0.2447, 0.6652],\n",
    "    #         [0.6652, 0.0900, 0.2447],\n",
    "    #         [0.0351, 0.2595, 0.7054]])\n",
    "    \n",
    "#验证一下\n",
    "# 这里我们将每一个输入变成了一个非负数。以外依据概率原理 ，每行的和为1\n",
    "X =  torch.normal(0, 1, (2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(1)\n",
    "# 因为将每个样本的类别预测概率之和设为1是对'概率分布的定义', 这里Softmax保证了这个特征，所以是被解释为一个概率分布\n",
    "\n",
    "#实现Softmax回归模型\n",
    "def net(X):\n",
    "    # 我们需要一个批量大小*输入维数的矩阵\n",
    "    # 这里-1表示自己计算相应的维度\n",
    "    # W.shape[0]是 784= 28*28\n",
    "    # 所以X就会被reshape成一个256*784的矩阵\n",
    "    # 然后对X和W进行matrix multiplicatioin也就是 256x784 x 784x10 = 256 x 10\n",
    "    # 通过‘广播机制’加上偏移\n",
    "    # 最后放进softmax得到'所有元素值>0'且'行和为1的输出'\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n",
    "    # 解释：1. X是一个批量的图像数据，用reshape(-1 和 W的维度将它变为一个2d的matrix\n",
    "    #      2. 因为这里W是‘权重矩阵’, 对于W而言, W[0]是输入特征的数量, W[1]是输出类别的数量\n",
    "    #      3. 在矩阵乘法后+偏置b. 因为这里的偏置b是一维向量, 长度等于类别的数量. 偏置b会通过广播机制传递到每一行\n",
    "    # 举例: 这里的b是[b1, b2, b3, ..., b10]\n",
    "    #      扩展后的b就是  [b1, b2, b3, ..., b10]\n",
    "    #                   [b1, b2, b3, ..., b10]\n",
    "    #                          ... (总共256)\n",
    "    #      也就是将对‘不同类别的偏置’复制了256份 ->广播机制\n",
    "\n",
    "#补充一个细节, 如何在预测值里面根据标号吧预测值拿出来\n",
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], \n",
    "                      [0.3, 0.2, 0.5]])\n",
    "y_hat[[0, 1], y]  # 给我们[0.1000, 0.5000]\n",
    "# 对第0号样本，拿出y第一个元素下标所对应的元素-> y_hat[0][0]\n",
    "# 对第1号样本，拿处y第二个元素下所标对应的元素-> y_hat[1][2]\n",
    "\n",
    "#实现交叉熵损失\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "    # 首先这里的range(len(y_hat))生成了一个[0,1,.....,n]的向量\n",
    "    # 这里y是包含每个样本真实索引的张量; 比如y=[2,1], 当索引是2,代表对应第(2+1)类, 是1,代表对应第(1+1)类;因为是从0开始的\n",
    "    # 更详细来说这里y是告诉我们真实类别在第几列\n",
    "    # 总的来说, 这里的y_hat像一个2d-matrix，里面每一行代表的是概率分布, 比如第一行是[0.3, 0.2, 0.5]\n",
    "    # 而这里的 y则是告诉我们每一行真实的下标, 比如这一行y是[1], 就是说0.2概率的类别是真实类别\n",
    "    # 所以说这一段是: 对每一行的‘概率分布’和‘真实类别的概率’做交叉熵损失\n",
    "    \n",
    "#将预测类别和真实元素y进行对比\n",
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and len(y_hat.shape[1]) >1: # 当y_hat是一个2-d matrix时\n",
    "        y_hat = y_hat.argmax(axis=1) # 将每一行元素值最大的下标, 存到y_hat里面\n",
    "        #这里y_hat就是预测分类的类别\n",
    "    cmp = y_hat.type(y.dtype) == y #这里有可能它们的数据类型不一样\n",
    "    # 1. 这里就将y_hat的数据类型转成y的数据类型\n",
    "    # 2. 对比y_hat的预测是不是与y一致\n",
    "    # 3. cmp是一个bool类型，这里存的是True，False表示预测正不正确\n",
    "    return float(cmp.type(y.dtype).sum()) \n",
    "    # 1. 将cmp转换为y的类型，其中True被转换为1，False被转换为0\n",
    "    # 2. 通过.sum()计算出正确预测的个数\n",
    "    \n",
    "accuracy(y_hat, y)/len(y) #计算出正确率\n",
    "\n",
    "# 我们可以评估在任意模型net的准确率\n",
    "def evaluate_accuracy(net, data_iter): #函数接受两个参数: net, data_iter\n",
    "    if  isinstance(net, torch.nn.Module): #这里'isinstance'用来检查net是不是Pytorch神经网络\n",
    "        net.eval() #将模型转换成'eval'模式\n",
    "    metric = Accumulator(2)\n",
    "    for X, y in data_iter: #从'data_iter'中迭代获取数据批次, 包括特征X和标签y\n",
    "        metric.add(accuracy(net(X), y), y.numel()) #net(X)对批次X进行预测, accuracy(net(X), y)函数计算预测准确的个数\n",
    "        #y.numel()返回当前批次中的样本量; 这些值被添加到metric积累器中\n",
    "    return metric[0]/metric[1] # 像我们前面提到的, accuracy是返回一个批次正确类别的个数, 存在metric[0]中, \n",
    "                               # metric[1]是储存当前批次的总数, 所以metric[0]/metric[1]就是正确率\n",
    "    \n",
    "# Accumulator的实现, 实例中创建了两个变量，用于存储正确预测的数量和样本总数\n",
    "class Accumulator:\n",
    "    #在n个变量上累加\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n # 创建了一个长为n的列表, 初始值为0.0\n",
    "    \n",
    "    def add(self, *args): # args*是Python中可变参数列表, 意味着你可以传递任意数量的参数给add方法\n",
    "        self.data = [a + float(b) for a,b in zip(self.data, args)] \n",
    "        # 这里的列表推列式用于生成新的列表数据\n",
    "        # zip(self.data, args) 中zip函数将self.data与args一一配对\n",
    "        # self.data存储累加器当前的值\n",
    "        # args是这次调用传入的一组新值\n",
    "        # zip函数创造了一个迭代器; 其中 a是来自self.data, b是来自args\n",
    "        # float(b) 确保了浮点类型\n",
    "        # a+float(b)对于每对(a,b)求和, a是累加器中的'当前值', b是传入的新值\n",
    "        # 总而言之, 这是用于更新累加器中的数据, 每次调用'add', 它都接受一组数值，并将这些数值逐一加到累加器的对应值上。\n",
    "        \n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data) #将所有值重置为0.0, 保持长度不变\n",
    "    \n",
    "    def __getitem__(self, idx): #用__getitem__使得实例Accumulator可以像列表那样被访问\n",
    "        return self.data[idx]\n",
    "\n",
    "# Softmax回归训练\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    if isinstance(net, torch.nn.Module): # 判断net是神经网络\n",
    "        net.train() # 训练网络\n",
    "    metric = Accumulator(3) # 这里的Metric有3个列表\n",
    "    for X, y in train_iter: \n",
    "        y_hat = net(X) # 这里得到y_hat为预测的标签\n",
    "        l = loss(y_hat, y) # 这里算残差值\n",
    "        if isinstance(updater, torch.optim.Optimizer): # 这里如果updater是Optimzer库的优化算法的话\n",
    "            updater.zero_grad() # 清除之前的梯度 \n",
    "            l.backward() # 对损失进行反向传播\n",
    "            updater.step() # 更新模型参数\n",
    "            metric.add(\n",
    "                float(1) * len(y), # 这里没啥用\n",
    "                accuracy(y_hat, y), # 累积模型的准确性\n",
    "                y.size().numel()) # 用于统计总样本数\n",
    "        else: # 如果这里的updater是自定义函数的话\n",
    "            l.sum().backward() # 对损失的总和进行反向传播\n",
    "            updater(X.shape[0]) # 使用批次大小调用更新函数\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "        return metric[0] / metric[2], metric[1] / metric[2]  # 平均损失 # 平均准确率\n",
    "\n",
    "# 定义一个在动画中绘制数据的实用程序类\n",
    "class Animator:\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                ylim=None, xscale='linear', yscale='linear', \n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                figsize=(3.5, 2.5)):\n",
    "        if legend is None:\n",
    "            legend =[]\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        self.config_axes = lambda: d2l.set_axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "        \n",
    "    def add(self, x, y):\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        \n",
    "# 训练函数\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, num_epochs],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, num_epochs, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch+1, train_metrics + (test_acc))\n",
    "    train_loss, train_acc = train.metrics\n",
    "    \n",
    "# 小批量随机梯度下降来优化模型的损失函数\n",
    "lr = 0.1\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W, b], lr, batch_size)\n",
    "\n",
    "#训练10个迭代周期\n",
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)\n",
    "\n",
    "#对图像进行分类预测\n",
    "def predict_ch3(net, test_iter, n=6):\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    titles= [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db707375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 回归的简单实现\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) #这里函数加载fashion_mnist数据集, 分为训练集和测试集\n",
    "\n",
    "# PyTorch不会隐式地调整输入的形状\n",
    "# 因此，我们定义了平展层(flattern)在线性层前调整网络输入的形状\n",
    "net = nn.Sequential(nn.Flattern(), nn.Linear(784, 10))\n",
    "# 这里输入的是256*28*28的批量数据, 然后通过平展层变为256*784\n",
    "# 这里的平展层是因为后面的Linear层的输入是一维的\n",
    "\n",
    "# 这里是一个函数, 定义了如何初始化模型的权重，它检查模型中的每一个模块，如果是'linear'模块，则对其weights进行初始化\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear: #判断传入的模块是否为Linear\n",
    "        nn.init.normal_(m.weight, std = 0.01) #使用标准差为0.01的normal dist来初始化线性层权重\n",
    "        \n",
    "net.apply(init_weights); # net.apply将'init_weights()'函数运用在net上的每一个模块\n",
    "# 为什么这里的init_weights不需要input? 因为.apply会自动处理这一块\n",
    "\n",
    "# 在交叉熵损失函数中传递未归一化预测\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 使用学习率为0.1的小批量随机梯度下降作为优化算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr =0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用之前定义的训练函数来训练模型\n",
    "num_epochs = 10\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
