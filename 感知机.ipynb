{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57dc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感知机的数学公式\n",
    "# 给定输出x, 权重w, 和偏移b, sign是符号函数, 用于映射+1/-1, 也就是 sign(x) = +1/-1\n",
    "# f(x) = sign(<w,x> + b); \n",
    "# 二分类：-1 或 +1\n",
    "#   vs. 回归输出实数\n",
    "#   vs. Softmax输出概率\n",
    "\n",
    "# 类别的意义\n",
    "# 在感知机模型中, 输出通常被标记为-1/+1, 使得数学处理变得简单，因为(*-1)可以表示方向调转\n",
    "#   +1 通常表示为一个正类\n",
    "#   -1 通常表示为一个负类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49147d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练感知机\n",
    "# initialize w = 0 and b = 0\n",
    "# repeat\n",
    "#   if y_i * [<w, x_i> + b] <= 0 then\n",
    "#      w = w + (y_i)(x_i) and b = b + y_i\n",
    "#   endif\n",
    "# until all classified correctly\n",
    "\n",
    "# 这里的 [<w, x_i> + b] 中表示是'决策分数', 这个分数用于决策判断, 可以是任意实数, 不局限于[0, 1]区间\n",
    "# y_i表示样本i的真实标签, 通常为+1/-1\n",
    "# 所以整个表达式 y_i * [<w, x_i> + b] 的值反应了当前模型对第i个样本分类的正确与否\n",
    "# 如果 y_i 和 [<w, x_i> + b] 的符号相同 (两者都是正数和负数), 则 y_i * [<w, x_i> + b] 会是正数, 意味着'分类正确'\n",
    "# 如果 y_i 和 [<w, x_i> + b] 的符号不同 (一个为正数, 一个为负数), 则 y_i * [<w, x_i> + b] 会是负数, 意味着'分类错误'\n",
    "\n",
    "# 这里的if条件表示分类不正确, 则执行更新\n",
    "# 更新权重w: w + (y_i)(x_i), 表示根据样本的实际类别y_i和它的特征向量x_i, 调整w\n",
    "# 因为这里已知'分类错误' \n",
    "# 如果y_i = +1 但是样本被预测为 -1, 意味着<w, x_i>+b 为负数, 小; \n",
    "# 增加w通过(y_i)(x_i)可以使得<w, x_i>增大, 则<w, x_i>+b变大; 同样b+y_i可以加速<w, x_i>增大的过程\n",
    "\n",
    "# 如果y_i = -1 但是样本被预测为 +1, 意味着<w, x_i>+b 为正数, 大; \n",
    "# 减小w通过(y_i_(x_i)可以使得<w, x_i>减小, 则<w, x_i>+b变小; 同样b-y_i可以加速<w, x_i>减少的过程\n",
    "\n",
    "# 总结: 已知这里分类错误, 通过正确的标签可以得知<w, x_i>+b的值是+/-; 通过对w做反向的调整, 可以使得预测正确; \n",
    "# 同时可以通过将b的值调整向y_i, 也就是正确类, 来加速这个update过程\n",
    "\n",
    "# 等价于使用批量大小为1的梯度下降, 并使用一下损失函数:\n",
    "# loss(y, x, w) = max(0, -y<w,x>)\n",
    "# 当 y*<w,x>分类正确时, 意味着y*<w,x>为+, 等同-y*<w,x>为-; 所以max() = 0无需更新\n",
    "# 当 y*<w,x>分类错误时, 意味着y*<w,x>为-, 等同-y*<w,x>为+; 所以max = -y*<w,x>, 需要更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3cb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收敛定理\n",
    "# 数据在半径r内, 这里的r是数据集中的所有点到到原点的最大欧氏距离, 简单来说r描述了数据集的大小\n",
    "# 余量p分为两类, y(x^Tw +b) >= p 对于 ||w||^2 + b^2 <= 1\n",
    "# 这里余量p是数据点到决策边界的最小距离, 也是分类余量. 这里y(x^Tw +b) >= p就表示所有的数据点都距离分类余量>=p的距离\n",
    "# ||w||^2 + b^2 <= 1是对模型参数的范数进行了约束\n",
    "\n",
    "# 感知机保证在 (r^2+1) / p^2 步之后收敛\n",
    "# 说明了感知机在 (r^2+1) / p^2 步之后一定收敛\n",
    "# 这里r表示数据点到原点的最远距离的square, r^2就表示数据的扩散情况\n",
    "# 这里p表示分类余量，当余量square p^2越大, 通常就表示数据点相对于决策边界更加分明, 感知机更容易学习\n",
    "# 感知机在(r^2+1) / p^2步后收敛\n",
    "\n",
    "# 但是感知机不能拟合XOR函数\n",
    "\n",
    "# 总结：感知机是一个二分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5956b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多重感知机\n",
    "# 解决XOR问题(如下)              1  2  3  4 \n",
    "#  gr1  |  rd2      y-axis     +  -  +  -\n",
    "#  ---  |  ---      x-axis     +  +  -  -\n",
    "#  rd3  |  gr4      product    +  -  -  +\n",
    "# 通过(AND)来得到product\n",
    "# 简单来讲就是一个单隐藏层(隐藏层大小是超参数) 因为输入/输出的维度不能改, 但是可以设置隐藏层大小\n",
    "# 输入 x in R^n\n",
    "# 隐藏层 W_1 in R^(mxn), b1 in R^m\n",
    "# 输出层 w_2 in R^(m)m b2 in R\n",
    "# h = sign(W_1x + b1)\n",
    "# o = (w_2)^T h + b2\n",
    "# sign是按元素的激活函数\n",
    "\n",
    "# 为什么需要一个非线性的激活函数 不能是sign(x) = x\n",
    "# 那么 o = (w_2)^T h + b2 = (w_2)^T (W_1x + b1) + b2 = ((w_2)^T W_1x)+b1 = (w'^T)x + b'; 也就是说仍然是线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9708a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数有 sigmoid函数 tanh函数 ReLu函数\n",
    "# sigmoid 将输入投影到(0,1) sigmoid(x) = 1/(1 + exp(-x))\n",
    "# tanh 将输入投影到(-1,1),  tanh(x) = 1-exp(-2x) / 1+exp(-2x)\n",
    "# ReLu是rectified linear unit, ReLu(x) = max(x, 0), 在 x<0时, gradient为0; 在x>0时, gradient为1\n",
    "# 其中ReLu算起来很容易\n",
    "\n",
    "# 多层分类\n",
    "# y1, y2,..., yl = softmax(o1, o2, ... , o_k)\n",
    "\n",
    "# 多层分类定义\n",
    "# 输入 x in R^n\n",
    "# 隐藏层 W_1 in R^(mxn), b_1 in R^m\n",
    "# 输出层 w_2 in R^(mxk), b_2 in R^k\n",
    "# h = sign(W_1 x + b1)\n",
    "# o = (W_2)^T h + b2\n",
    "# y = softmax(o)\n",
    "# 和之前没什么区别, 除了k, 因为输出有k个单元\n",
    "# 这就是做多类分类的\n",
    "\n",
    "# 多隐藏层\n",
    "# h1 = sign(W_1x + b1)\n",
    "# h2 = sign(W_2x + b2)\n",
    "# h3 = sign(W_3x + b3)\n",
    "# o = (W_4)(h_3) + b4\n",
    "# 超参数有: 隐藏层数, 每个隐藏层大小\n",
    "# 一般来说: 多隐藏层中每一个隐藏层都比上面小一点; 或者第一层先大一点\n",
    "# 注意: 需要有激活函数每层, 不然会有层数塌陷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2dfdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结: 多层感知机使用隐藏层和激活函数来得到非线性模型\n",
    "# 使用softmax来处理多分类\n",
    "# 超参数为层数和每个层数的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93dc8e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md2l\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m d2l\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m      6\u001b[0m train_iter, test_iter \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mload_data_fashion_mnist(batch_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/d2l/torch.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751d04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个具有单隐藏层的多重感知机, 它包含256个隐藏单元\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256 #输入是28*28的图片, 输出是10个种类\n",
    "W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True)) #输入是28*28的展平后的数据, 输出256个隐藏单元\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens), requires_grad=True) #bias为一个256的偏置\n",
    "W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True)) #输入是256个隐藏单元, 输出是10个种类\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs), requires_grad=True) #bias为一个10的偏置\n",
    "\n",
    "params = [W1, b1, W2, b2] #这里是所有的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5ee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现ReLu函数的激活\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)\n",
    "\n",
    "# 实现我的模型\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs)) # X进来先把它拉成一个二维的矩阵, 然后num_inputs就是一行为一个数据\n",
    "    H = relu(X @ W1 + b1) # 这里的@符号是乘法, X是一个 batch_size * 784, W1是一个784 * 256, 得到 H是一个batch_size * 256\n",
    "    return (H @ W2 + b2) # 最后 H是一个 batch_size * 10, W2是一个256 * 10, 得到最后的是 batch_size * 10\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94298014",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      3\u001b[0m updater \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(params, lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m----> 4\u001b[0m \u001b[43md2l\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "# 多层感知机的训练过程与Softmax回归的训练完全相同\n",
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4383dd51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m---> 14\u001b[0m train_iter, test_iter \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241m.\u001b[39mload_data_fashion_mnist(batch_size)\n\u001b[1;32m     15\u001b[0m d2l\u001b[38;5;241m.\u001b[39mtrain_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "# 简洁实现\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std = 0.1)\n",
    "\n",
    "net.apply(init_weights);\n",
    "\n",
    "batch_size, lr, num_epochs = 256, 0.1, 10\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886df923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
