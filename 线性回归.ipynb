{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6472bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear Regresion是对n-维的加权，加上一个偏差\n",
    "# 2. 用平方差来估量预测值和真实值的差异\n",
    "# 3. LR 有显示解\n",
    "# 4. LR 可以看做单层神经网络\n",
    "# 有 X = [x1,x2,...,xn]^T, y = [y1,y2,...,yn]^T, n个数据\n",
    "# Loss = 1/2n |y-Xw-b|^2, 用 w*, b* = argmin loss来学习参数\n",
    "# 将加入权重为 X = [X, 1], w = [w, b]^T\n",
    "# dLoss/dw = 1/n (y - Xw)^T X； 当=0时，得到最小值，为 w* = (X^T X)^-1 X^T y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86742754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降\n",
    "# 将 w0 作为随机初始值，迭代t=1,2,3\n",
    "# wt = wt-1 - n*(dLoss/dw), 因为dLoss/dw是增加最快的方向，-dLoss/dw是减少最快的方向\n",
    "# n作为步长，可理解为为走多远\n",
    "# 批量下降，随机采样b个样本为一个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9562fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将图嵌入到notebook\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "644ac1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造一个数据集，用 w=[2,-3.4]^T, b=4.2, y = Xw + b + 噪声\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 随机数, 有n个样本，列数为w个\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape) #加入随机噪声\n",
    "    return X, y.reshape((-1, 1))\n",
    "                     \n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000) # feature为因素，label为预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebc8e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-0.4982, -0.4757]) \n",
      "labels: tensor([4.8117])\n"
     ]
    }
   ],
   "source": [
    "# features中每一行是一个2-d vector, labels中每一行是一个标量\n",
    "print('features:', features[0], '\\nlabels:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "402ec967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1511,  0.4517],\n",
      "        [-1.3180, -1.0271],\n",
      "        [-1.1956, -0.6494],\n",
      "        [ 2.0913,  1.8373],\n",
      "        [-1.0065, -0.8676],\n",
      "        [ 0.1223,  0.1020],\n",
      "        [-0.3477,  1.8908],\n",
      "        [ 1.5729, -1.6319],\n",
      "        [-0.2688, -0.5206],\n",
      "        [-0.7008,  1.6389]]) \n",
      " tensor([[ 2.9897],\n",
      "        [ 5.0553],\n",
      "        [ 4.0398],\n",
      "        [ 2.1305],\n",
      "        [ 5.1201],\n",
      "        [ 4.0943],\n",
      "        [-2.9217],\n",
      "        [12.9057],\n",
      "        [ 5.4162],\n",
      "        [-2.7718]])\n"
     ]
    }
   ],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    #随机读取样本\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 得到一个batch的indices\n",
    "        batch_indices = torch.tensor(\n",
    "        indices[i:min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices] #相当于return\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6a962a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义初始化参数\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3cbe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d76a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "def square_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 /2 / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da975ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化算法\n",
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad(): #更新时不要参与梯度计算\n",
    "        for param in params:\n",
    "            param -= lr * param.grad \n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e744a2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1, loss0.000006\n",
      "epoch2, loss0.000006\n",
      "epoch3, loss0.000006\n"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = square_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y) # 算小批量损失\n",
    "        # 因为'l'的形状时('batch_size', 1), 要sum所有的元素\n",
    "        l.sum().backward()\n",
    "        sgd([w,b], lr, batch_size)\n",
    "    with torch.no_grad():\n",
    "        #扫完之后，评价一下进度, 与真实的值做对比\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch{epoch+1}, loss{float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf3880a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差:tensor([0.0001, 0.0002], grad_fn=<SubBackward0>)\n",
      "b的估计误差:tensor([-0.0003], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#比较真实参数与训练参数的区别\n",
    "print(f'w的估计误差:{true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差:{true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61702da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5233f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size, shuffle \u001b[38;5;241m=\u001b[39m is_train)\n\u001b[1;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 9\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[43mload_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_iter))\n",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m, in \u001b[0;36mload_array\u001b[0;34m(data_arrays, batch_size, is_train)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_array\u001b[39m(data_arrays, batch_size, is_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#构造一个PyTorch数据迭代器\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mTensorDataset(\u001b[38;5;241m*\u001b[39mdata_arrays) \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#这里的*data_arrays是解包操作符，允许从一个列表或原组中解包并传递多个参数，这样是允许TensorDataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size, shuffle \u001b[38;5;241m=\u001b[39m is_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#调用现有框架中的API来读取数据\n",
    "def load_array(data_arrays, batch_size, is_train = True):\n",
    "    #构造一个PyTorch数据迭代器\n",
    "    dataset = data.TensorDataset(*data_arrays) \n",
    "    #这里的*data_arrays是解包操作符，允许从一个列表或原组中解包并传递多个参数，这样是允许TensorDataset接受一对或多对特征和标签张量\n",
    "    return data.DataLoader(dataset, batch_size, shuffle = is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "next(iter(data_iter)) #转成python的iterator\n",
    "#这行是用迭代器换取下一个元素，这里iter()是确保这里是一个迭代器，next()是从中获取第一个批次的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ddc66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对模型的定义\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1)) #这里2是输入的维度，1是输出的维度\n",
    "#这里Sequential是list of layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8fcc5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化模型参数\n",
    "net[0].weight.data.normal_(0, 0.01) \n",
    "#这里的 net[0]访问到具体的layer, weight来访问到w，\n",
    "#data是真实data，normal是正太分布来替换掉具体的值\n",
    "net[0].bias.data.fill_(0)\n",
    "#fill是把这里全部设置成0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1911c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用均方误差\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09fb3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化SGD\n",
    "trainer = torch.optim.SGD(net.parameters(), lr =0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5a6e11d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m data_iter: \u001b[38;5;66;03m#这里X是feature, y是label\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         l \u001b[38;5;241m=\u001b[39m loss(net(X), y) \u001b[38;5;66;03m#net(X)表示feature X通过net得到的预测结果\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m#告诉tranier先把梯度清零\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter: #这里X是feature, y是label\n",
    "        l = loss(net(X), y) #net(X)表示feature X通过net得到的预测结果\n",
    "        trainer.zero_grad() #告诉tranier先把梯度清零\n",
    "        l.backward()\n",
    "        trainer.step() #对模型的更新\n",
    "    l = loss(net(features), labels) #这行代码通过计算整个数据集上的损失来评估模型的性能\n",
    "    print(f'epoch {epoch + 1}, loss{l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e3098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
